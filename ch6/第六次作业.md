# 第六次作业





## 第二题：LK光流



### 2.1 光流文献综述



#### 2.2.1 光流法的分类

​		各种方法之间的区分之一是：按照估计参数是 加性增量 以及 估计增量warp。

​		按照Warp的更新规则又可以将光流法分为 前向 和 逆向 两种算法

#### 2.2.2 在 compositional 中，为什么有时候需要做原始图像的 warp?该 warp 有何物理意义？

​			

Lucas-Kanade 算法中将迭代更新的 Δ*p* 添加到估计参数 $p$ 

组合算法则中的更新（*W*(*x*;Δ*p*)）则是则由warp的当前估计（*W*(*x*;p)）组成，需要在当前位姿估计前，引入增量式建立半群约束要求。

warp可以提高算法的鲁棒性

#### 2.2.3 forward 和 inverse 有何差别？



如果模板中有很多噪声，则应该使用正向算法。如果输入图像中噪声更多，那就采用逆向算法。逆向算法只用计算一次Hessian矩阵，而正向算法需要在每次迭代过程中都计算一遍Hessian矩阵。因此，逆向算法更高效，计算量较小。



### 2.2 forward-addtive Gauss-Newton 光流的实现



#### 2.2.1 从最小二乘角度来看，每个像素的误差怎么定义？

![image-20221011211030059](/home/cp/.config/Typora/typora-user-images/image-20221011211030059.png)

$I_1$是图一中像素点的灰度值，$I_2$是图二中的坐标对应的像素点的灰度值



#### 2.2.2 误差相对于自变量的导数如何定义？



由于图像的像素是离散的，所以采用中心差分的方式求导。其结果为：

![image-20221011212913070](/home/cp/.config/Typora/typora-user-images/image-20221011212913070.png)

#### 2.2.3 代码实现



```c++
// 计算 Jacobian
J = -1.0 * Eigen::Vector2d(
 0.5*( GetPixelValue(img2, kp.pt.x + x+ dx + 1, kp.pt.y+y+dy) - GetPixelValue(img2, kp.pt.x + x + dx-1 , kp.pt.y + y + dy) ),
 0.5*( GetPixelValue(img2, kp.pt.x + x+dx, kp.pt.y+y+dy +1) - GetPixelValue(img2, kp.pt.x + x + dx, kp.pt.y + y + dy-1) )
    
error = -double(GetPixelValue(img2,kp.pt.x + x + dx,kp.pt.y + y+dy) - GetPixelValue(img1,kp.pt.x + x ,kp.pt.y + y));

// 计算 Hession 矩阵  及 误差   
 H += J * J.transpose();
 b += -error * J;
 cost += error * error;      
    
 update = H.ldlt().solve(b);
```



运行结果



![image-20221013170756530](/home/cp/.config/Typora/typora-user-images/image-20221013170756530.png)



### 2.3 反向法



反向的光流法与之前的优点在于，利用$I_1$处的梯度，替换掉原本要计算的$I_2$处的梯度

相应的导数为：

![image-20221011220129246](/home/cp/.config/Typora/typora-user-images/image-20221011220129246.png)





```c++
J(0) = -0.5 * (img2.at<uchar>(kp.pt.x+dx+x+1, kp.pt.y+dy+y)   - img2.at<uchar>(kp.pt.x+dx+x-1, kp.pt.y+dy+y));
J(1) = -0.5 * (img2.at<uchar>(kp.pt.x+dx+x,   kp.pt.y+dy+y+1) - img2.at<uchar>(kp.pt.x+dx+x,   kp.pt.y+dy+y-1));
error = -double(img2.at<uchar>(kp.pt.x + x + dx , kp.pt.y + y+dy) - img1.at<uchar>(kp.pt.x + x, kp.pt.y + y));
```



![](/home/cp/.config/Typora/typora-user-images/image-20221016205135558.png)

![image-20221016205149475](/home/cp/.config/Typora/typora-user-images/image-20221016205149475.png)

### 2.4  推广至金字塔

```c++
    // TODO START YOUR CODE HERE (~8 lines)
    for (int i = 0; i < pyramids; i++) {
        if(i==0)
        {
            pyr1.push_back(img1);
            pyr2.push_back(img2);
        }
        else
        {
            Mat dst1,dst2;
            cv::resize(img1, dst1, Size(),scales[i],scales[i]);
            cv::resize(img2, dst2, Size(),scales[i],scales[i]);
            pyr1.push_back(dst1);
            pyr2.push_back(dst2);
            /* code */
        }
    }
    // TODO END YOUR CODE HERE
    vector<KeyPoint> kp1_pyr;
    vector<KeyPoint> kp2_pyr;
    for(auto &kp:kp1)
    {
        auto kp_top=kp;
        kp_top.pt *= scales[pyramids-1];
        kp1_pyr.push_back(kp_top);
        kp2_pyr.push_back(kp_top);
    }
    // coarse-to-fine LK tracking in pyramids
    // TODO START YOUR CODE HERE
    for(int level = pyramids-1;level>=0;level--)
    {
        success.clear();
        cout<<pyr1.size()<<' '<<level<<endl;
        OpticalFlowSingleLevel(pyr1[level], pyr2[level], kp1_pyr, kp2_pyr, success,true);
        if(level>0)
        {
            for(auto &kp:kp1_pyr)
            {
                kp.pt /= pyramid_scale;
            }
            for(auto &kp:kp2_pyr)
            {
                kp.pt /= pyramid_scale;
            }
        }
    }
    // TODO END YOUR CODE HERE
    // don't forget to set the results into kp2
    for(auto &kp:kp2_pyr)
    {
        kp2.push_back(kp);
    }
```





![](/home/cp/.config/Typora/typora-user-images/image-20221016205108182.png)

![image-20221016205154239](/home/cp/.config/Typora/typora-user-images/image-20221016205154239.png)

### 2.5  并行化













### 2.6 讨论



#### 2.6.1 我们优化两个图像块的灰度之差真的合理吗？哪些时候不够合理？你有解决办法吗？



光流法有三个假设，灰度不变假设、小运动假设、局部一致性假设。



通过优化两个图像块的灰度只差来判断移动是建立在 **灰度不变假设** 的基础上的。这一假设成立条件较为苛刻，极易受到类似高光、阴影、材质等因素的影响，很可能不成立。





#### 2.6.2 图像块大小是否有明显差异？取 16x16 和 8x8 的图像块会让结果发生变化吗？





当采用金字塔时，窗口固定，将图像生成金字塔过程中，在每一层金字塔上都用同一个大小的窗口来进行光流计算，这样很好的去解决了图像块的问题，这样一来图像块大小并不会带来明显差异。

当不使用金字塔方法时。当窗口较大时，光流计算更鲁棒，当窗口较小时，光流计算更正确。原因在于，当图像中每一个部分的运动都不一致的时候如果开的窗口过大，很容易违背窗口(邻域)内的所有点光流一致的基本假设，这可能与实际不一致，所以窗口小，包含的像素少，更精确些。



#### 2.6.3 金字塔层数对结果有怎样的影响？缩放倍率呢？



金字塔层数一般越多效果越好，但是一般图像大于4~5层之后都变得太小，特征点像素太过紧密容易出现错误追踪。放大倍率的话，放大倍率小，金字塔的层数可以增加，迭代层数增多，效果可以变得更好。



## 第三题：直接法





### 3.1 单层直接法



#### 3.1.1 该问题中的误差项是什么？


$$
error=I_{ref}(p_1)-I_{cur}(p_2)
$$
![image-20221012154944023](/home/cp/.config/Typora/typora-user-images/image-20221012154944023.png)



理解为，事先不知道点 P 1 P_1 P1对应哪个 P 2 P_2 P2，根据当前相机位置的估计值来寻找点 P 2 P_2 P2的位置，进而通过对比两个点的光度误差来优化相机位姿。



#### 3.3.2 误差相对于自变量的雅可比维度是多少？如何求解？

![image-20221012163112023](/home/cp/.config/Typora/typora-user-images/image-20221012163112023.png)



#### 3.3.3 窗口可以取多大？是否可以取单个点？



取单点的化，精度会有所下降。



### 代码



```c++
void DirectPoseEstimationSingleLayer(
        const cv::Mat &img1,
        const cv::Mat &img2,
        const VecVector2d &px_ref,
        const vector<double> depth_ref,
        Sophus::SE3 &T21
) {

    // parameters
    int half_patch_size = 4;
    int iterations = 100;

    double cost = 0, lastCost = 0;
    int nGood = 0;  // good projections
    VecVector2d goodProjection;
    ///用于画图对应
    VecVector2d goodRef;

    for (int iter = 0; iter < iterations; iter++) {
        nGood = 0;
        goodProjection.clear();
        goodRef.clear();

        // Define Hessian and bias
        Matrix6d H = Matrix6d::Zero();  // 6x6 Hessian
        Vector6d b = Vector6d::Zero();  // 6x1 bias

        for (size_t i = 0; i < px_ref.size(); i++) {

            // compute the projection in the second image
            // TODO START YOUR CODE HERE
            double tempXw = (px_ref[i][0] - cx)/fx*depth_ref[i];
            double tempYw = (px_ref[i][1] - cy)/fy*depth_ref[i];
            Eigen::Matrix<double, 3, 1> pw(tempXw,tempYw,depth_ref[i]);
            Eigen::Matrix<double, 3, 1> pw2 = T21*pw;
            if (pw2[2] < 0)   /// depth invalid
                continue;

            double u = fx*pw2[0]/pw2[2]+cx;
            double v = fy*pw2[1]/pw2[2]+cy;
            if (u < half_patch_size || u > img2.cols - half_patch_size || v < half_patch_size || v > img2.rows - half_patch_size)
                continue;

            nGood++;
            goodProjection.push_back(Eigen::Vector2d(u, v));
            goodRef.push_back(Eigen::Vector2d(px_ref[i][0], px_ref[i][1]));

            double X = pw2[0], Y = pw2[1], Z = pw2[2], Z_inv = 1.0 / Z, Z2_inv = Z_inv * Z_inv;

            // and compute error and jacobian
            for (int x = -half_patch_size; x < half_patch_size; x++)
                for (int y = -half_patch_size; y < half_patch_size; y++) {

                    double error = GetPixelValue(img1, px_ref[i][0] + x, px_ref[i][1] + y) -
                                   GetPixelValue(img2, u + x, v + y);

                    Matrix26d J_pixel_xi;   // pixel to \xi in Lie algebra
                    Eigen::Vector2d J_img_pixel;    // image gradients

                    J_pixel_xi(0, 0) = fx * Z_inv;
                    J_pixel_xi(0, 1) = 0;
                    J_pixel_xi(0, 2) = -fx * X * Z2_inv;
                    J_pixel_xi(0, 3) = -fx * X * Y * Z2_inv;
                    J_pixel_xi(0, 4) = fx + fx * X * X * Z2_inv;
                    J_pixel_xi(0, 5) = -fx * Y * Z_inv;

                    J_pixel_xi(1, 0) = 0;
                    J_pixel_xi(1, 1) = fy * Z_inv;
                    J_pixel_xi(1, 2) = -fy * Y * Z2_inv;
                    J_pixel_xi(1, 3) = -fy - fy * Y * Y * Z2_inv;
                    J_pixel_xi(1, 4) = fy * X * Y * Z2_inv;
                    J_pixel_xi(1, 5) = fy * X * Z_inv;

                    J_img_pixel = Eigen::Vector2d(
                            0.5 * (GetPixelValue(img2, u + 1 + x, v + y) - GetPixelValue(img2, u - 1 + x, v + y)),
                            0.5 * (GetPixelValue(img2, u + x, v + 1 + y) - GetPixelValue(img2, u + x, v - 1 + y))
                    );

                    // total jacobian
                    Vector6d J = -1.0 * (J_img_pixel.transpose() * J_pixel_xi).transpose();

                    H += J * J.transpose();
                    b += -error * J;
                    cost += error * error;
                }
            // END YOUR CODE HERE
        }

        // solve update and put it into estimation
        // TODO START YOUR CODE HERE
        Vector6d update = H.ldlt().solve(b);;
        T21 = Sophus::SE3::exp(update) * T21;
        // END YOUR CODE HERE

        cost /= nGood;

        if (isnan(update[0])) {
            // sometimes occurred when we have a black or white patch and H is irreversible
            cout << "update is nan" << endl;
            break;
        }
        if (iter > 0 && cost > lastCost) {
            cout << "cost increased: " << cost << ", " << lastCost << endl;
            break;
        }
        lastCost = cost;
        cout << "cost = " << cost << ", good = " << nGood << endl;
    }
    cout << "good projection: " << nGood << endl;
    cout << "T21 = \n" << T21.matrix() << endl;

    // in order to help you debug, we plot the projected pixels here
    cv::Mat img1_show, img2_show;
    cv::cvtColor(img1, img1_show, CV_GRAY2BGR);
    cv::cvtColor(img2, img2_show, CV_GRAY2BGR);
    for (auto &px: px_ref) {
        cv::rectangle(img1_show, cv::Point2f(px[0] - 2, px[1] - 2), cv::Point2f(px[0] + 2, px[1] + 2),
                      cv::Scalar(0, 250, 0));
    }
//    for (auto &px: goodProjection) {
//        cv::rectangle(img2_show, cv::Point2f(px[0] - 2, px[1] - 2), cv::Point2f(px[0] + 2, px[1] + 2),
//                      cv::Scalar(0, 250, 0));
//    }
    for (size_t i = 0; i < goodProjection.size(); ++i) {
        auto p_ref = goodRef[i];
        auto p_cur = goodProjection[i];

        if (p_cur[0] > 0 && p_cur[1] > 0) {
            cv::circle(img2_show, cv::Point2f(p_cur[0], p_cur[1]), 1, cv::Scalar(0, 250, 0), 2);
            cv::line(img2_show, cv::Point2f(p_ref[0], p_ref[1]), cv::Point2f(p_cur[0], p_cur[1]),
                     cv::Scalar(0, 255, 0));
        }
    }

    cv::imwrite("reference.bmp", img1_show);
    cv::imwrite("current.bmp", img2_show);

    cv::imshow("reference", img1_show);
    cv::imshow("current", img2_show);
    cv::waitKey();
}
```





### 运行结果

![](/home/cp/.config/Typora/typora-user-images/image-20221013194532924.png)



### 3.2 多层直接算法



### 代码



```c++
         for (int i = 0; i < pyramids; i++) {
            if (i == 0) {
                pyr1.push_back(img1);
                pyr2.push_back(img2);
            } else {
                cv::Mat img1_pyr, img2_pyr;
                cv::resize(pyr1[i - 1], img1_pyr,
                           cv::Size(pyr1[i - 1].cols * pyramid_scale, pyr1[i - 1].rows * pyramid_scale));
                cv::resize(pyr2[i - 1], img2_pyr,
                           cv::Size(pyr2[i - 1].cols * pyramid_scale, pyr2[i - 1].rows * pyramid_scale));
                pyr1.push_back(img1_pyr);
                pyr2.push_back(img2_pyr);
            }
        }


        fx = fxG * scales[level];
        fy = fyG * scales[level];
        cx = cxG * scales[level];
        cy = cyG * scales[level];

        // END YOUR CODE HERE
        DirectPoseEstimationSingleLayer(pyr1[level], pyr2[level], px_ref_pyr, depth_ref, T21);
    }
```



### 演示结果



![image-20221013195850117](/home/cp/.config/Typora/typora-user-images/image-20221013195850117.png)











### 3.3 并行化













### 3.4 延伸讨论



#### 3.4.1 直接法是否可以类似光流，提出 inverse, compositional 的概念？它们有意义吗？



直接法通过求解相机运动来计算第二帧图像对应关键点的位置，如果提出 inverse, compositional 的概念，第二帧图像的信息将会失效，没有意义。



#### 3.4.2  请思考上面算法哪些地方可以缓存或加速？

加窗口部分，采用合适大小的窗口会使计算加速。图像梯度可以提前计算好。



#### 3.4.3 在上述过程中，我们实际假设了哪两个 patch 不变？

1. 灰度不变假设
2. 同窗口内深度不变假设



#### 3.4.4 为何可以随机取点？而不用取角点或线上的点？那些不是角点的地方，投影算对了吗？



​		直接法对图像中的灰度值优化，最小化的是目标像素块的是光度误差，不是角点也没有关系。而特征点法提取的特征点是为了能够进行图像匹配，而在图像中选择具有代表性的区域



#### 3.4.5 请总结直接法相对于特征点法的异同与优缺点。



​		直接法最大的贡献在于，以更整体、更优雅的方式处理了数据关联问题。特征点法需要依赖重复性较强的特征提取器，以及正确的特征匹配，才能得正确地计算相机运动。而直接法，则并不要求一一对应的匹配，只要先前的点在当前图像当中具有合理的投影残差，我们就认为这次投影是成功的。而成功与否，主要取决于我们对地图点深度以及相机位姿的判断，并不在于图像局部看起来是什么样子。



## 第四题：使用光流法计算视差



对代码的理解不到位，做不出来。。。

```c++
vector<double > disp;
vector<double > cost;
for(int i = 0 ;i<pixels_ref.size();i++)
{
    double dis =  pixels_ref[i].x()-pixel_right[i].x(); //两点之间的水平视差
    disp.push_back(dis);
    //        cout<<"dis: "<<dis<<" "<<GetPixelValue(disparity_img,pixels_ref[i].x(),pixels_ref[i].y())<<endl;
    cost.push_back(dis-GetPixelValue(disparity_img,pixels_ref[i].x(),pixels_ref[i].y()));
}
//计算视差的平均误差
double sum = accumulate(cost.begin(),cost.end(),0.0);
cout<<"平均误差:"<<sum/cost.size()<<endl;
```

















































